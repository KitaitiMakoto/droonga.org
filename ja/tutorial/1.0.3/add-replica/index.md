---
title: "Droonga tutorial: 既存クラスタへのreplicaの追加"
layout: ja
---

{% comment %}
##############################################
  THIS FILE IS AUTOMATICALLY GENERATED FROM
  "_po/ja/tutorial/1.0.3/add-replica/index.po"
  DO NOT EDIT THIS FILE MANUALLY!
##############################################
{% endcomment %}


* TOC
{:toc}

## チュートリアルのゴール

既存の[Droonga][]クラスタについて、新しいreplicaを追加し、既存のreplicaを削除し、および、既存のreplicaを新しいreplicaで置き換えるための手順を学ぶこと。

## 前提条件

* You must have an existing Droonga cluster with some data.
  Please complete the ["getting started" tutorial](../groonga/) before this.
* You must know how to duplicate data between multiple clusters.
  Please complete the ["How to backup and restore the database?" tutorial](../dump-restore/) before this.

## 「replica」とは？

Droongaのノードの集合には、「replica」と「slice」という2つの軸があります。

「replica」のノード群は、完全に同一のデータを持っており、検索などのリクエストを各ノードが並行して処理する事ができます
新しいreplicaを追加する事によって、増加するリクエストに対して処理能力を増強することができます。

他方、「slice」のノード群はそれぞれ異なるデータを持ちます（例えば、あるノードは2013年のデータ、別のノードは2014年のデータ、という具合です）。
新しいsliceを追加する事によって、増大するデータ量に対してクラスタとしての容量を拡大することができます。

現在の所、Groonga互換のシステムとして設定されたDroongaクラスタについては、replicaを追加することはできますが、sliceを追加することはできません。
この点については将来のバージョンで改善する予定です。

ともかく、このチュートリアルでは既存のDroongaクラスタに新しいreplicaを追加する手順を解説します。
早速始めましょう。

## 既存のクラスタに新しいreplicaノードを追加する

このケースでは、検索のように読み取りのみを行うリクエストに対しては、クラスタの動作を止める必要はありません。
サービスを停止することなく、その裏側でシームレスに新しいreplicaを追加することができます。

その一方で、クラスタへの新しいデータの流入は、新しいノードが動作を始めるまでの間停止しておく必要があります。
（将来的には、新しいノードを完全に無停止で追加できるようにする予定ですが、今のところはそれはできません。）

ここでは、`192.168.0.10` と `192.168.0.11` の2つのreplicaノードからなるDroongaクラスタがあり、新しいreplicaノードとして `192.168.0.12` を追加すると仮定します。

### 新しいノードをセットアップする

まず、新しいコンピュータをセットアップし、必要なソフトウェアのインストールと設定を済ませます。

    (on 192.168.0.12)
    # apt-get update
    # apt-get -y upgrade
    # apt-get install -y ruby ruby-dev build-essential nodejs npm
    # gem install droonga-engine
    # npm install -g droonga-http-server
    # mkdir ~/droonga

ここで、以前にクラスタを構築する時に `catalog.json` を生成するために実行したコマンド列を思い出して下さい:

    (on 192.168.0.10 or 192.168.0.11)
    # droonga-engine-catalog-generate --hosts=192.168.0.10,192.168.0.11 \
                                      --output=~/droonga/catalog.json

新しいノード用には、`--host` オプションの値以外はすべて同じ指定で、単一のノードだけを含む `catalog.json` を生成します:

    (on 192.168.0.12)
    # droonga-engine-catalog-generate --hosts=192.168.0.12 \
                                      --output=~/droonga/catalog.json

では、サーバを起動しましょう。

    (on 192.168.0.12)
    # host=192.168.0.12
    # droonga-engine --host=$host \
                     --daemon \
                     --pid-file=~/droonga/droonga-engine.pid
    # droonga-http-server --port=10041 \
                          --receive-host-name=$host \
                          --droonga-engine-host-name=$host \
                          --daemon \
                          --pid-file=~/droonga/droonga-http-server.pid

この時点で、2つの別々のDroongaクラスタが存在するようになりました。

 * 2つのreplicaを含む既存のクラスタ。以下、*「alpha」*と仮称します。
   * `192.168.0.10`
   * `192.168.0.11`
 * 1つのreplicaを含む新しいクラスタ。以下、*「beta」*と仮称します。
   * `192.168.0.12`

### 書き込みを伴うリクエストの流入を一時的に停止する

クラスタ alpha とクラスタ beta のデータを完全に同期する必要があるので、データの複製を始める前に、クラスタ alphaへのデータの書き込みを行うリクエストの流入を一時停止する必要があります。
そうしないと、新しく追加したreplicaが中途半端なデータしか持たない状態となってしまいます。
replica同士の内容に矛盾があると、リクエストに対してクラスタが返す処理結果が不安定になります。

データの書き込みを伴うリクエストとは、具体的には、クラスタ内のデータを変更する以下のコマンドです:

 * `add`
 * `column_create`
 * `column_remove`
 * `delete`
 * `load`
 * `table_create`
 * `table_remove`

cronjobとして実行されるバッチスクリプトによって `load` コマンド経由で新しいデータを投入している場合は、cronjobを停止して下さい。
クローラが `add` コマンド経由で新しいデータを投入している場合は、クローラを停止して下さい。
あるいは、クローラやローダーとクラスタの間にFluentdを置いてバッファとして利用しているのであれば、バッファからのメッセージ出力を停止して下さい。 

### 既存のクラスタから新しいreplicaへデータを複製する

クラスタ alpha からクラスタ beta へデータを複製します。
これは `drndump` と `droonga-request` の各コマンドを使って行います。
（Gemパッケージ `drndump` と `droonga-client` をあらかじめインストールしておいて下さい。）

    (on 192.168.0.12)
    # drndump --host=192.168.0.10 \
              --receiver-host=192.168.0.12 | \
        droonga-request --host=192.168.0.12 \
                        --receiver-host=192.168.0.12

`--receiver-host` オプションに作業マシン自身のホスト名またはIPアドレスを指定しておく必要がある事に注意して下さい。
ノード `192.168.0.11` の上で作業する場合であれば、コマンド列は以下の通りです:

    (on 192.168.0.11)
    # drndump --host=192.168.0.10 \
              --receiver-host=192.168.0.11 | \
        droonga-request --host=192.168.0.12 \
                        --receiver-host=192.168.0.11

### 新しいreplicaをクラスタに参加させる

データを正しく複製できたら、新しいreplicaを既存のクラスタに参加させます。
新たにkる明日谷参加するノード `192.168.0.12` 上で、すべてノードを `--hosts` オプションに指定して `catalog.json` を再作成してください:

    (on 192.168.0.12)
    # droonga-engine-catalog-generate --hosts=192.168.0.10,192.168.0.11,192.168.0.12 \
                                      --output=~/droonga/catalog.json

すると、サーバのプロセスが新しい `catalog.json` を検知して、自動的に自分自身を再起動させます。

この時点で、理論上、部分的に重なり合う2つのDroongaクラスタが存在するようになりました。

 * 2つのreplicaを含む既存のクラスタ「alpha」。
   * `192.168.0.10`
   * `192.168.0.11`
 * 3つのreplicaを含む新しいクラスタ。以下、*「charlie」*と仮称します。
   * `192.168.0.10`
   * `192.168.0.11`
   * `192.168.0.12`

「beta」と仮称した一時的なクラスタが姿を消している事に注意してください。
この時、新しいノード `192.168.0.12` はクラスタ charlie が3つのノードを含んでいる事を知っていますが、他の2つの既存のノードはその事を知りません。
既存の2つのノードは、自分自身が属しているクラスタ内にいるノードは2つだけだと認識しているため、流入してきたリクエストは、新しいノード `192.168.0.12` へはまだ配送されません。

次に、新しい `catalog.json` を`192.168.0.12` から他のノードにコピーします。

    (on 192.168.0.12)
    # scp ~/droonga/catalog.json 192.168.0.10:~/droonga/
    # scp ~/droonga/catalog.json 192.168.0.11:~/droonga/

コピー先のノードのサーバが新しい `catalog.json` を認識して、自動的に再起動します。

この時点で、Droongaクラスタは1つだけ存在する状態となっています。

 * 3つのreplicaを含む新しいクラスタ「charlie」。
   * `192.168.0.10`
   * `192.168.0.11`
   * `192.168.0.12`

「alpha」と仮称した古いクラスタが姿を消している事に注意してください。
この時、2つのreplicaからなる古いクラスタの代わりとして、新しいクラスタ「charlie」は3つのreplicaのもとで完璧に動作しています。

### 書き込みを伴うリクエストの流入を再開する

さて、準備ができました。
すべてのreplicaは完全に同期した状態となっているので、このクラスタはリクエストを安定して処理できます。
cronjobを有効化する、クローラの動作を再開する、バッファからのメッセージ送出を再開する、などの操作を行って、クラスタ内のデータを変更するリクエストの流入を再開して下さい。

以上で、Droongaクラスタに新しいreplicaノードを無事参加させる事ができました。


## Remove an existing replica node from an existing cluster

A Droonga node can die by various fatal reasons - for example, OOM killer, disk-full error, troubles around its hardware, etc.
Because nodes in a Droonga cluster observe each other and they stop delivering messages to dead nodes automatically, the cluster keeps working even if there are some dead nodes.
Then you have to remove dead nodes from the cluster.

Of course, even if a node is still working, you may plan to remove it to reuse for another purpose.

Assume that there is a Droonga cluster constructed with trhee replica nodes `192.168.0.10`, `192.168.0.11` and `192.168.0.12`, and planning to remove the last node `192.168.0.12` from the cluster.

### Unjoin an existing replica from the cluster

To remove a replica from an existing cluster, you just have to update the "catalog.json" with new list of replica nodes except the node to be removed:

    (on 192.168.0.10)
    # droonga-engine-catalog-generate --hosts=192.168.0.10,192.168.0.11 \
                                      --output=~/droonga/catalog.json

この時点で、理論上、部分的に重なり合う2つのDroongaクラスタが存在するようになりました。

 * The existing cluster "charlie" including three replicas.
   * `192.168.0.10`
   * `192.168.0.11`
   * `192.168.0.12`
 * The new cluster including two replicas.
   Let's give a name *"delta"* to it, for now.
   * `192.168.0.10`
   * `192.168.0.11`

The node `192.168.0.10` with new `catalog.json` knows the cluster delta includes only two nodes, so it doesn't deliver incoming messages to the missing node `192.168.0.12` anymore.

Next, copy new `catalog.json` from `192.168.0.10` to others.

    (on 192.168.0.10)
    # scp ~/droonga/catalog.json 192.168.0.11:~/droonga/
    # scp ~/droonga/catalog.json 192.168.0.12:~/droonga/

Then there is only one Droonga cluster on this time.

 * The new cluster "delta" including two replicas.
   * `192.168.0.10`
   * `192.168.0.11`

Even if both nodes `192.168.0.11` and `192.168.0.12` receive requests, they are delivered to the nodes of the cluster delta.
The orphan node `192.168.0.12` never process requests by self.

OK, the node is ready to be removed.
Stop servers and shutdown it if needed.

    (on 192.168.0.12)
    # kill $(cat ~/droonga/droonga-engine.pid)
    # kill $(cat ~/droonga/droonga-http-server.pid)

## Replace an existing replica node in a cluster with a new one

Replacing of nodes is a combination of those instructions above.

Assume that there is a Droonga cluster constructed with two replica nodes `192.168.0.10` and `192.168.0.11`, the node `192.168.0.11` is unstable, and planning to replace it with a new node `192.168.0.12`.

### Unjoin an existing replica from the cluster

First, remove the unstable node.
Re-generate `catalog.json` without the node to be removed, and spread it to other nodes in the cluster:

    (on 192.168.0.10)
    # droonga-engine-catalog-generate --hosts=192.168.0.10 \
                                      --output=~/droonga/catalog.json
    # scp ~/droonga/catalog.json 192.168.0.11:~/droonga/

After that the node `192.168.0.11` unjoins from the cluster successfully.

### Add a new replica

Next, setup the new replica.
Construct a temporary cluster with only one node, and duplicate data from the existing cluster:

    (on 192.168.0.12)
    # droonga-engine-catalog-generate --hosts=192.168.0.12 \
                                      --output=~/droonga/catalog.json
    # drndump --host=192.168.0.10 \
              --receiver-host=192.168.0.12 | \
        droonga-request --host=192.168.0.12 \
                        --receiver-host=192.168.0.12

After the duplication successfully finished, the node is ready to join the cluster.
Re-generate `catalog.json` and spread it to all nodes in the cluster:

    (on 192.168.0.12)
    # droonga-engine-catalog-generate --hosts=192.168.0.10,192.168.0.12 \
                                      --output=~/droonga/catalog.json
    # scp ~/droonga/catalog.json 192.168.0.10:~/droonga/

Finally a Droonga cluster constructed with two nodes `192.168.0.10` and `192.168.0.12` is here.


## まとめ

In this tutorial, you did add a new replica node to an existing [Droonga][] cluster.
Moreover, you did remove an existing replica, and did replace a replica with a new one.

  [Ubuntu]: http://www.ubuntu.com/
  [Droonga]: https://droonga.org/
  [Groonga]: http://groonga.org/
  [command reference]: ../../reference/commands/
